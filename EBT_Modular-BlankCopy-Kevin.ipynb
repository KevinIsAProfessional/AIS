{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Meeting Notes:\n",
    "    Modularity \n",
    "    Data issues: \n",
    "        Availability of absence data\n",
    "        Gaps between year of available data\n",
    "    Best platform for end-user tool\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Environment Set up\n",
    "\n",
    "conda create -n gee python=3\n",
    "source activate gee\n",
    "conda install -c conda-forge earthengine-api\n",
    "conda install -c anaconda pandas\n",
    "#After installing the Python GEE API, you have to run earthengine authenticate in the terminal and follow the directions. This will connect the API to your google account.\n",
    "\n",
    "#After running earthengine authenticate, you should have an environment variable set up with an authentication key, which allows you to directly initialize EE without authenticating each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import ee and required packages\n",
    "import ee\n",
    "ee.Initialize()\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import glob\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLE DECLARATIONS\n",
    "\n",
    "STATE=\"Montana\"\n",
    "\n",
    "\n",
    "\n",
    "state_abbrevs = {\n",
    "    'Montana' : 'MT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING AREA\n",
    "print(STATE)\n",
    "print(state_abbrevs[STATE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#If you need to create the spatially thinned asset...Otherwise skip to Define Modular Variables below\n",
    "#Define GEE asset/location of desired dataset (Formatted CSV must be uploaded to your GEE assets with Lat/Long columns defined \n",
    "#before starting)\n",
    "Taxa_og = ee.FeatureCollection('users/kjchristensen93/EBT_data/EBT_mfish_data_presence_heuristic')\n",
    "coll = ee.FeatureCollection(Taxa_og) \n",
    "distance = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Spatially thin locations and export to asset\n",
    "#//========================================================\n",
    "\n",
    "def filterDistance(points, distance):\n",
    "    ## EE FUNCTION THAT TAKES A FEATURE COLLECTION AND FILTERS \n",
    "    ## BY A MINIMUM DISTANCE IN METERS\n",
    "    def iter_func(el, ini):\n",
    "        ini = ee.List(ini)\n",
    "        fcini = ee.FeatureCollection(ini)\n",
    "        buf = ee.Feature(el).geometry().buffer(distance)\n",
    "        s = fcini.filterBounds(buf).size()\n",
    "        cond = s.lte(0)\n",
    "        return ee.Algorithms.If(cond, ini.add(el), ini)\n",
    "    filt2 = ee.List([])\n",
    "    filt = points.iterate(iter_func, filt2)\n",
    "    filtered = ee.FeatureCollection(ee.List(filt))\n",
    "    return filtered\n",
    "\n",
    "#//========================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function that filters the feature collection and spatially thins it\n",
    "def filter_date_space(date):\n",
    "    start_date = ee.Date(date).advance(1, 'year')\n",
    "    end_date = start_date.advance(1, 'year')\n",
    "    points_in_that_year = coll.filterDate(start_date, end_date)\n",
    "    \n",
    "    spatially_filt = filterDistance(points_in_that_year, distance)\n",
    "    \n",
    "    return spatially_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Spatially thin locations and export to asset\n",
    "# Performs the spatial thinning algorithm on each year separately\n",
    "feats = s_dates.map(filter_date_space)\n",
    "def merge_coll(first_year, second_year):\n",
    "    return ee.FeatureCollection(first_year).merge(ee.FeatureCollection(second_year))\n",
    "\n",
    "# Combine each of the resultant filtered collections\n",
    "first = ee.FeatureCollection(Taxa_og)\n",
    "spatially_thin = ee.FeatureCollection(feats.iterate(merge_coll, first))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "export3 = ee.batch.Export.table.toAsset(collection = spatially_thin,\n",
    "                    description = 'EBT_SThin', # n<-------- CHANGE NAME FOR DIFFERENT DATA\n",
    "                    assetId = 'users/kjchristensen93/EBT_SThin') # <----- CHANGE Export location FOR DIFFERENT USER\n",
    "\n",
    "export3.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Modular Variables:\n",
    "\n",
    "#If you have a spatially thinned data set, start here after initializing ee\n",
    "\n",
    "#Taxa thinned dataset\n",
    "SThin = ee.FeatureCollection('users/mstokowski/EBT_SThin')\n",
    "#Study dates\n",
    "#Note we are limited to 2002 - 2018 due to the water year covariate \n",
    "s_dates = ee.List ([ \n",
    "ee.Date('2002-01-01'),\n",
    "ee.Date('2003-01-01'),ee.Date('2004-01-01'),\n",
    "ee.Date('2005-01-01'),ee.Date('2006-01-01'),\n",
    "ee.Date('2007-01-01'),ee.Date('2008-01-01'),\n",
    "ee.Date('2009-01-01'),ee.Date('2010-01-01'),\n",
    "ee.Date('2011-01-01'),ee.Date('2012-01-01'),\n",
    "ee.Date('2013-01-01'),ee.Date('2014-01-01'),\n",
    "ee.Date('2015-01-01'),ee.Date('2016-01-01'),\n",
    "ee.Date('2017-01-01'),ee.Date('2018-01-01')]) \n",
    "\n",
    "#HUC state geojson file \n",
    "HUC_state = ('C:/Users/YOURFOLDER/YOUR_STATE_HUC.geojson')\n",
    "#Define export locations:\n",
    "#GEE yearly covariate folder\n",
    "assetId = ('users/kjchristensen93/covariates_test') \n",
    "#User training csv local directory folder\n",
    "trainingdata = ('/Users/myles/Documents/flbs')\n",
    "#User decadal image local directory folder\n",
    "decadalfolder = ('C:/Users/YOUR_DECADE_FOLDER_PATH')\n",
    "#Define export naming convention? Maybe we define a function within code above for naming conventions\n",
    "\n",
    "\n",
    "#### ML Variables ####\n",
    "\n",
    "#Training Glob\n",
    "trainingglob = ('/Users/myles/Documents/*.csv')\n",
    "# trainingglob = ((trainingdata)/*.csv) will this work?\n",
    "#decadal CSV directory and naming conventions\n",
    "decade1 = ('/Users/myles/Documents/flbs/decade1_filename.csv')\n",
    "decade2 =('/Users/myles/Documents/flbs/decade2.filename.csv')\n",
    "#decadal predictions\n",
    "decade1_pred = ('/Users/myles/Documents/flbs/decade1_pred_filename.csv')\n",
    "decade2_pred = ('/Users/myles/Documents/flbs/decade2_pred_filename.csv')\n",
    "\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This list dictates what years will be exported for both the Yearly Covariate Images and the Yearly Training CSVS\n",
    "# can this be changed to a list for intermitent datasets missing years? Empty outputs causes issues later on....\n",
    "import time\n",
    "# Enter start year for Y and end year for Y\n",
    "years = [str(y) for y in list(range(2002, 2005))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Export data using python API magic\n",
    "# Define geometry by changing state name so we can export the whole state at once\n",
    "states = ee.FeatureCollection(\"TIGER/2016/States\")\n",
    "#Enter state 2-digit abbreviation for study area\n",
    "geometry = states.filter(ee.Filter.eq('NAME',STATE)).geometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape file containing HUC polygons\n",
    "HUC = ee.FeatureCollection(\"USGS/WBD/2017/HUC12\")\n",
    "# Choose state to clip HUC by. Change Abbreviation to match dataset \n",
    "#Enter state full name for X (i.e., Illinois/ look at dataset for formats for this stuff)\n",
    "HUC_clip = HUC.filter(ee.Filter.eq('states',state_abbrevs[STATE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed observation Year as system:start_time for thinned dataset \n",
    "# We have had to add this \"Year Column\" manually to the datasets.  Make sure your dataset has correct column headings\n",
    "def embedd_date(x):\n",
    "    yr = ee.Number(x.get(\"Year\"))\n",
    "    eedate = ee.Date.fromYMD(yr, 1, 1)\n",
    "    return x.set(\"system:time_start\", eedate)\n",
    "SThin_map = SThin.map(embedd_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Big Raster Image\n",
    "## Import assets\n",
    "# MODIS Mission\n",
    "modusGlobal = ee.ImageCollection(\"MODIS/006/MYD11A2\")\n",
    "\n",
    "# Primary Productivity\n",
    "GPP = ee.ImageCollection(\"UMT/NTSG/v2/LANDSAT/GPP\")\n",
    "\n",
    "# Surface water\n",
    "pikelSurfaceWater = ee.Image(\"JRC/GSW1_1/GlobalSurfaceWater\")\n",
    "\n",
    "# Elevation\n",
    "DEM = ee.Image(\"USGS/NED\")\n",
    "\n",
    "# Enhanced Vegetation Index and NDVI\n",
    "modusVeg = ee.ImageCollection(\"MODIS/006/MYD13A2\")\n",
    "\n",
    "# Heat Isolation Load\n",
    "CHILI = ee.Image(\"CSP/ERGo/1_0/Global/SRTM_CHILI\")\n",
    "\n",
    "# Topographic Diversity\n",
    "topoDiversity = ee.Image(\"CSP/ERGo/1_0/Global/ALOS_topoDiversity\")\n",
    "\n",
    "# Vegetation Continuous Field product - percent tree cover, etc\n",
    "VCF = ee.ImageCollection(\"MODIS/006/MOD44B\")\n",
    "\n",
    "# Human Modification index\n",
    "gHM = ee.ImageCollection(\"CSP/HM/GlobalHumanModification\")\n",
    "\n",
    "# Climate information\n",
    "NLDAS = ee.ImageCollection(\"NASA/NLDAS/FORA0125_H002\")\n",
    "\n",
    "# Shape file containing Country Boundaries\n",
    "countries = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\")\n",
    "\n",
    "# Shape file containing HUC polygons\n",
    "HUC = ee.FeatureCollection(\"USGS/WBD/2017/HUC12\")\n",
    "\n",
    "# Dynamic Surface Water metric\n",
    "pekel_monthly_water = ee.ImageCollection(\"JRC/GSW1_2/MonthlyHistory\")\n",
    "\n",
    "# Static surface water metric\n",
    "pekel_static_water = ee.ImageCollection('JRC/GSW1_2/MonthlyRecurrence')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Select features, etc\n",
    "#========================================================\n",
    "#Rename Bands and select bands, etc\n",
    "#========================================================\n",
    "\n",
    "\n",
    "NLDAS_precip = NLDAS.select(\"total_precipitation\");\n",
    "NLDAS_temp = NLDAS.select(\"temperature\");\n",
    "NLDAS_humid = NLDAS.select(\"specific_humidity\");\n",
    "NLDAS_potEvap = NLDAS.select(\"potential_evaporation\");\n",
    "\n",
    "\n",
    "CHILI = CHILI.rename(['Heat_Insolation_Load'])\n",
    "srtmChili = CHILI.select('Heat_Insolation_Load');\n",
    "topoDiversity = topoDiversity.rename([\"Topographic_Diversity\"])\n",
    "topoDiv = topoDiversity.select(\"Topographic_Diversity\")\n",
    "footprint = ee.Image(gHM.first().select(\"gHM\"));\n",
    "\n",
    "# Surface water occurrence\n",
    "sw_occurrence = pekel_static_water\\\n",
    "                      .select('monthly_recurrence')\\\n",
    "                      .mean()\\\n",
    "                      .rename(['SurfaceWaterOccurrence'])\\\n",
    "                      .unmask()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Define helper filters and lists to iterate over\n",
    "#========================================================\n",
    "# Build Lists from which to map over\n",
    "#========================================================\n",
    "# List from which absences will be built\n",
    "ee_dates = ee.List(s_dates)\n",
    "\n",
    "\n",
    "# Required for exporting presence locations due to covariate availability \n",
    "ee_dates_presence = ee.List(s_dates) ##FIXME: Optimize. We run the same function twice, could clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Mask features by quality control bands\n",
    "# ========================================================\n",
    "# Masking TPP via quality control bands\n",
    "# ========================================================\n",
    "def gpp_qc(img):\n",
    "    img2 = img.rename(['GPP','QC']);\n",
    "    quality = img2.select(\"QC\")\n",
    "    mask = quality.neq(11) \\\n",
    "                .And(quality.neq(10)) \\\n",
    "                .And(quality.neq(20)) \\\n",
    "                .And(quality.neq(21)) \n",
    "    return img2.mask(mask)\n",
    "\n",
    "GPP_QC = GPP.map(gpp_qc);\n",
    "\n",
    "# ========================================================\n",
    "# Masking LST via quality control bands\n",
    "# ========================================================\n",
    "def lst_qc(img):\n",
    "    quality = img.select(\"QC_Day\")\n",
    "    mask = quality.bitwiseAnd(3).eq(0) \\\n",
    "                .And(quality.bitwiseAnd(12).eq(0))\n",
    "    return img.mask(mask)\n",
    "\n",
    "LST = modusGlobal.map(lst_qc) \\\n",
    "                 .select(\"LST_Day_1km\");\n",
    "\n",
    "        \n",
    "# ========================================================\n",
    "# Mask Modus Vegetation Indices by quality flag\n",
    "# ========================================================\n",
    "def modusQC(image):\n",
    "    quality = image.select(\"SummaryQA\")\n",
    "    mask = quality.eq(0)\n",
    "    return image.updateMask(mask)\n",
    "\n",
    "modusVeg_QC = modusVeg.map(modusQC)\n",
    "EVI = modusVeg_QC.select(\"EVI\")\n",
    "NDVI = modusVeg_QC.select(\"NDVI\")\n",
    "\n",
    "# ========================================================\n",
    "# Mask Continuous Fields via quality control band\n",
    "# ========================================================\n",
    "def VCFqc(img):\n",
    "    quality = img.select(\"Quality\")\n",
    "    mask = quality.bitwiseAnd(2).eq(0) \\\n",
    "                    .And(quality.bitwiseAnd(4).eq(0)) \\\n",
    "                    .And(quality.bitwiseAnd(8).eq(0)) \\\n",
    "                    .And(quality.bitwiseAnd(16).eq(0)) \\\n",
    "                    .And(quality.bitwiseAnd(32).eq(0))\n",
    "\n",
    "    return img.mask(quality)\n",
    "\n",
    "VCF_qc = VCF.map(VCFqc)\n",
    "\n",
    "\n",
    "#========================================================\n",
    "# Define Point Joins such that each HUC contains a list of observational data:\n",
    "#========================================================\n",
    "distFilter = ee.Filter.intersects(**{\n",
    "  'leftField': '.geo', \n",
    "  'rightField': '.geo', \n",
    "  'maxError': 100\n",
    "});\n",
    "\n",
    "pointJoin = ee.Join.saveAll(**{\n",
    "  'matchesKey': 'Points',\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Annual Cube function\n",
    "#========================================================\n",
    "# \"Builder Function\" -- processes each annual variable into a list of images\n",
    "#========================================================\n",
    "\n",
    "def build_annual_cube(d):\n",
    "    # Set start and end dates for filtering time dependent predictors (SR, NDVI, Phenology)\n",
    "      # Advance startDate by 1 to begin with to account for water year (below)\n",
    "    startDate = (ee.Date(d).advance(1.0,'year').millis()) ## FIXME: Why do we advance a year? this give 2003-2019 instead of 2002-2018\n",
    "    endDate = ee.Date(d).advance(2.0,'year').millis()\n",
    "\n",
    "  #========================================================\n",
    "  #Define function to compute seasonal information for a given variable\n",
    "  #========================================================\n",
    "    def add_seasonal_info(imgCol,name,bandName):\n",
    "        winter = imgCol.filterDate(winter_start,winter_end)\n",
    "        spring = imgCol.filterDate(spring_start,spring_end)\n",
    "        summer = imgCol.filterDate(summer_start,summer_end)\n",
    "        fall = imgCol.filterDate(fall_start,fall_end)\n",
    "\n",
    "        winter_tot = winter.sum()\n",
    "        spring_tot = spring.sum()\n",
    "        summer_tot = summer.sum()\n",
    "        fall_tot = fall.sum()\n",
    "\n",
    "        winter_max = winter.max()\n",
    "        winter_min = winter.min()\n",
    "        spring_max = spring.max()\n",
    "        spring_min = spring.min()\n",
    "        summer_max = summer.max()\n",
    "        summer_min = summer.min()\n",
    "        fall_max = fall.max()\n",
    "        fall_min = fall.min()\n",
    "\n",
    "        winter_diff = winter_max.subtract(winter_min)\n",
    "        spring_diff = spring_max.subtract(spring_min)\n",
    "        summer_diff = summer_max.subtract(summer_min)\n",
    "        fall_diff = fall_max.subtract(fall_min)\n",
    "\n",
    "        names = ['winter_total'+name,'spring_total'+name,'summer_total'+name,\n",
    "                      'fall_total'+name]\n",
    "\n",
    "        return winter_tot.addBands([spring_tot,summer_tot,fall_tot]) \\\n",
    "                         .rename(names)\n",
    "\n",
    "  # Set up Seasonal dates for precip, seasonal predictors\n",
    "    winter_start = ee.Date(startDate)\n",
    "    winter_end = ee.Date(startDate).advance(3,'month')\n",
    "    spring_start = ee.Date(startDate).advance(3,'month')\n",
    "    spring_end = ee.Date(startDate).advance(6,'month')\n",
    "    summer_start = ee.Date(startDate).advance(6,'month')\n",
    "    summer_end = ee.Date(startDate).advance(9,'month')\n",
    "    fall_start = ee.Date(startDate).advance(9,'month')\n",
    "    fall_end = ee.Date(endDate)\n",
    "\n",
    "  # Aggregate seasonal info for each variable of interest (potEvap neglected purposefully)\n",
    "    seasonal_precip = add_seasonal_info(NLDAS_precip,\"Precip\",\"total_precipitation\")\n",
    "    seasonal_temp = add_seasonal_info(NLDAS_temp,\"Temp\",\"temperature\")\n",
    "    seasonal_humid = add_seasonal_info(NLDAS_humid,\"Humidity\",\"specific_humidity\")\n",
    "\n",
    "    waterYear_start = ee.Date(startDate).advance(10,'month')\n",
    "    waterYear_end = waterYear_start.advance(1,'year')\n",
    "\n",
    "  #========================================================\n",
    "  # Aggregate Other Covariates\n",
    "  #========================================================\n",
    "\n",
    "  # Vegetative Continuous Fields\n",
    "    meanVCF = VCF.filterDate(startDate, endDate)\\\n",
    "                 .mean()\n",
    "    \n",
    "#     VCF_qc.filterDate(startDate, endDate) \\\n",
    "#                       .mean()\n",
    "\n",
    "  # Filter Precip by water year to get total precip annually\n",
    "\n",
    "    waterYearTot = NLDAS_precip.filterDate(waterYear_start,waterYear_end) \\\n",
    "                                 .sum()\n",
    "\n",
    "  # Find mean EVI per year:\n",
    "    maxEVI = EVI.filterDate(startDate,endDate) \\\n",
    "                  .mean() \\\n",
    "                  .rename(['Mean_EVI'])\n",
    "\n",
    "  #Find mean NDVI per year:\n",
    "    maxNDVI = NDVI.filterDate(startDate,endDate) \\\n",
    "                    .mean() \\\n",
    "                    .rename([\"Mean_NDVI\"])\n",
    "\n",
    "  # Find flashiness per year by taking a Per-pixel Standard Deviation:\n",
    "    flashiness_yearly = ee.Image(pekel_monthly_water.filterDate(startDate,endDate) \\\n",
    "                                                      .reduce(ee.Reducer.sampleStdDev()) \\\n",
    "                                                      .select([\"water_stdDev\"])) \\\n",
    "                                                      .rename(\"Flashiness\")\n",
    "\n",
    "  # Find max LST per year:\n",
    "    maxLST = LST.max().rename([\"Max_LST_Annual\"])\n",
    "\n",
    "  # Find mean GPP per year:\n",
    "    maxGPP = GPP_QC.filterDate(startDate,endDate) \\\n",
    "                      .mean() \\\n",
    "                      .rename(['Mean_GPP','QC'])\n",
    "\n",
    "  # All banded images that don't change over time\n",
    "    static_input_bands = sw_occurrence.addBands(DEM.select(\"elevation\")) \\\n",
    "                                          .addBands(srtmChili) \\\n",
    "                                          .addBands(topoDiv) \\\n",
    "                                          .addBands(footprint)\n",
    "\n",
    "  # Construct huge banded image\n",
    "    banded_image = static_input_bands \\\n",
    "                          .addBands(srcImg = maxLST, names = [\"Max_LST_Annual\"]) \\\n",
    "                          .addBands(srcImg = maxGPP, names = [\"Mean_GPP\"]) \\\n",
    "                          .addBands(srcImg =  maxNDVI, names = [\"Mean_NDVI\"]) \\\n",
    "                          .addBands(srcImg = maxEVI, names = [\"Mean_EVI\"]) \\\n",
    "                          .addBands(meanVCF.select(\"Percent_Tree_Cover\")) \\\n",
    "                          .addBands(seasonal_precip) \\\n",
    "                          .addBands(flashiness_yearly) \\\n",
    "                          .set(\"system:time_start\",startDate)\n",
    "\n",
    "    return banded_image.unmask()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#========================================================\n",
    "# REDUCE REGIONS FUNCTION\n",
    "#========================================================\n",
    "def reduce_HUCS(img):\n",
    "    # Cast to image because EE finicky reasons\n",
    "    img = ee.Image(img)\n",
    "    startDate = ee.Date(img.get(\"system:time_start\"))\n",
    "    endDate = startDate.advance(1,'year')\n",
    "    \n",
    "    # Find each occurrence record within the year\n",
    "    pointsInThatYear = SThin_map.filterDate(startDate, endDate);\n",
    "    \n",
    "    # Apply spatial join to presence data \n",
    "    new_HUCS = pointJoin.apply(HUC_clip,pointsInThatYear,distFilter);\n",
    "    \n",
    "    #Reduce the Multi-band image to HUC-level aggregates\n",
    "    reduced_image = img.reduceRegions(**{\n",
    "                              'collection': new_HUCS,\n",
    "                              'reducer': ee.Reducer.mean(),\n",
    "                              'crs': 'EPSG:4326',\n",
    "                              'scale': 100,\n",
    "                              'tileScale': 16}).map(lambda y:y.set({'Time': img.get(\"system:time_start\")}))\n",
    "    \n",
    "    def getAvgPresence (feat):\n",
    "        pts = ee.List(feat\n",
    "              .get('Points')) \\\n",
    "              .map(lambda pt: ee.Feature(pt).get('Present'))\n",
    "                          \n",
    "        avg = ee.List(pts).reduce(ee.Reducer.mean())\n",
    "   \n",
    "        return feat.set(\"Avg_Presence\", avg)\n",
    "    \n",
    "    return reduced_image.map(getAvgPresence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#========================================================\n",
    "# Run covariate algorithm and build a list of images\n",
    "# with each image corresponding to each year and each band corresponding to each covariate\n",
    "#========================================================\n",
    "\n",
    "# Image Collection\n",
    "banded_images = ee.ImageCollection(ee_dates.map(build_annual_cube))\n",
    "\n",
    "# List form\n",
    "banded_images_list = ee.List(ee_dates.map(build_annual_cube))\n",
    "\n",
    "annual_stacks = ee.FeatureCollection(banded_images.map(reduce_HUCS))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Skip this step if you already have them stored in GEE\n",
    "#Export Yearly Covariate Images\n",
    "\n",
    "# Export each image within the for loop\n",
    "for i,y in zip(range(len(years)), years):\n",
    "    print(\"Starting\", y)\n",
    "    img = ee.Image(ee.List(banded_images_list).get(ee.Number(i)))\n",
    "    export = ee.batch.Export.image.toAsset(image = img,\n",
    "                    description = 'covariate_'+y,\n",
    "                    assetId = (assetId) +y, \n",
    "                    region = ee.Geometry(geometry),\n",
    "                    scale =  100,\n",
    "                    maxPixels = 1e13)\n",
    "    export.start()\n",
    "    \n",
    "    print(y,\"status:    \", export.status()['state'])\n",
    "\n",
    "    # Wait for 30 seconds so that the export['state'] gives insightful information\n",
    "    time.sleep(15)\n",
    "    print(y,\"status:    \", export.status()['state'])\n",
    "    \n",
    "    \n",
    "    # If this status is \"RUNNING\", then there are no egretious syntax errors. \n",
    "    # However, it is still possible that these export commands fail after more than 30 seconds.\n",
    "    # In that case, it is likely that there is a Computation Time Out Error (remember exporting the annual stacks)\n",
    "    time.sleep(15)\n",
    "    print(y,\"status:    \", export.status()['state'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Start Here if you have yearly covariates created\n",
    "\n",
    "#Export training CSVs\n",
    "## Reduce Regions from existing images\n",
    "\n",
    "#Can we set this up such that this is automatically defined when we define the year range above?\n",
    "\n",
    "# COVARIATE IMAGES  \n",
    "\n",
    "path = 'users/kjchristensen93/covariates_test'\n",
    "years = range(2002, 2005)\n",
    "images = map(lambda x: ee.Image(path + str(x)), years)\n",
    "banded_images_asset_list = ee.List(list(images))\n",
    "\n",
    "for i in range(len(years)):\n",
    "    print(\"Starting\", i)\n",
    "    \n",
    "    img = ee.Image(banded_images_asset_list.get(i))\n",
    "    data = reduce_HUCS(img) \n",
    "    \n",
    "    ## PYTHON API MAGIC!! LOOK HERE\n",
    "    my_csv = pd.DataFrame([x['properties'] for x in data.getInfo()['features']])\n",
    "    \n",
    "    # From there, we can write it directly to our directory and stitch it together afterwards\n",
    "    my_csv.to_csv((trainingdata) + str(2002+i) + '.csv', index=False) \n",
    "    print(\"Finished\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export the information that we will use to project habitat suitability. \n",
    "#Decades were convenient for RBT, but not other taxa with less data/ we can change.\n",
    "# Change to match dataset\n",
    "\n",
    "#Can we set this up such that this is automatically defined when we define the year range above?\n",
    "first_decade = ee.ImageCollection.fromImages([image1, \n",
    "                                              image2, \n",
    "                                              #image3, \n",
    "                                              #image4,\n",
    "                                              #image5, \n",
    "                                              #image6, \n",
    "                                              #image7, \n",
    "                                              #image8\n",
    "                                                ]).mean()\n",
    "\n",
    "#second_decade = ee.ImageCollection.fromImages([image9, \n",
    "                                               #image10, \n",
    "                                               #image11, \n",
    "                                               #image12, \n",
    "                                               #image13, \n",
    "                                               #image14, \n",
    "                                               #image15\n",
    "                                                  #]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Export these data as csvs\n",
    "first_decade_img = ee.Image(first_decade)\n",
    "\n",
    "first_csv = first_decade_img.reduceRegions(**{\n",
    "                              'collection': HUC_clip,\n",
    "                              'reducer': ee.Reducer.mean(),\n",
    "                              'crs': 'EPSG:4326',\n",
    "                              'scale': 100,\n",
    "                              'tileScale': 16})\n",
    "\n",
    "#PYTHON API MAGIC!! LOOK HERE\n",
    "first_decade_data = pd.DataFrame([x['properties'] for x in first_csv.getInfo()['features']])\n",
    "\n",
    "# From there, we can write it directly to our directory and stitch it together afterwards\n",
    "#maybe we should think about 2 and 5 year bins due to limitations of datasets for some taxa/ to make more useful for managers\n",
    "first_decade_data.to_csv(decade1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Export these data as csvs for second decade if dataset contains that many sampling years\n",
    "second_decade_img = ee.Image(second_decade)\n",
    "\n",
    "second_csv = second_decade_img.reduceRegions(**{\n",
    "                              'collection': HUC_clip,\n",
    "                              'reducer': ee.Reducer.mean(),\n",
    "                              'crs': 'EPSG:4326',\n",
    "                              'scale': 100,\n",
    "                              'tileScale': 16})\n",
    "\n",
    "## PYTHON API MAGIC!! LOOK HERE\n",
    "second_decade_data = pd.DataFrame([x['properties'] for x in second_csv.getInfo()['features']])\n",
    "\n",
    "# From there, we can write it directly to our directory and stitch it together afterwards\n",
    "second_decade_data.to_csv(decade2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined data\n",
    "\n",
    "dfs = []\n",
    "for file in glob.glob(trainingglob):\n",
    "    dfs.append(pd.read_csv(file))\n",
    "\n",
    "df = pd.DataFrame(pd.concat(dfs, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Points', 'areaacres', 'areasqkm', \n",
    "           'gnis_id','huc12', 'humod','hutype',\n",
    "           'loaddate', 'metasource', 'name', \n",
    "           'noncontr00','noncontrib','shape_area',\n",
    "           'shape_leng', 'sourcedata','sourcefeat',\n",
    "           'sourceorig','states','tnmid', 'tohuc', 'Time']\n",
    "\n",
    "df['Avg_Presence'] = [1 if x > 0 else 0 for x in df['Avg_Presence']]\n",
    "df.drop(columns = to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Train Models\n",
    "\n",
    "#install skLearn if you don't already have it\n",
    "\n",
    "# Ensemble modeling class\n",
    "from sklearn.metrics import *\n",
    "class Ensemble():\n",
    "    def __init__(self, models = []):\n",
    "        self.models = models\n",
    "        self.accs_dict = None\n",
    "        self.rocs_dict = None\n",
    "        self.model_names = [m.__class__.__name__ for m in models]\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit_all(self, X_train, y_train):\n",
    "        for m in self.models:\n",
    "            print(\"Fitting\", m.__class__.__name__)\n",
    "            m.fit(X_train, y_train)\n",
    "            print(m.__class__.__name__, 'fit.')\n",
    "        \n",
    "    \n",
    "    def evaluate_all(self, X_test, y_true, metric = 'acc'):\n",
    "        accs = [accuracy_score(y_true, m.predict(X_test)) for m in self.models]\n",
    "        accs_dict = dict(zip(self.model_names, accs))\n",
    "        \n",
    "        \n",
    "        rocs = [roc_auc_score(y_true, m.predict(X_test)) for m in self.models]\n",
    "        rocs_dict = dict(zip(self.model_names, rocs))\n",
    "        \n",
    "        self.rocs_dict = rocs_dict\n",
    "        self.accs_dict = accs_dict\n",
    "        \n",
    "        if metric == 'acc':\n",
    "            return accs_dict\n",
    "        \n",
    "        return rocs_dict\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.rocs_dict\n",
    "    \n",
    "    def get_model_names(self):\n",
    "        return self.model_names\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = ['Avg_Presence']),\n",
    "                                                   df['Avg_Presence'], random_state = 73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "\n",
    "# Here is the ensemble of models - again, I think the tree methods are far stronger\n",
    "mlp = MLPClassifier(max_iter = 1000, random_state = 73)\n",
    "logit = LogisticRegression(max_iter = 10000)\n",
    "rf = RandomForestClassifier()\n",
    "brt = GradientBoostingClassifier()\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Construct ensemble object\n",
    "ensemble = Ensemble([mlp, brt, dt, rf, logit]) \n",
    "ensemble.fit_all(X_train,y_train)\n",
    "\n",
    "print(ensemble.evaluate_all(X_test, y_test, metric = 'roc'))\n",
    "#y_pred = ensemble.evaluate_ensemble(X_test, y_test)\n",
    "\n",
    "#accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Test classifyers for ideal settings with this function: Change model names and parameters\n",
    "#n_estimators doesn't affect score\n",
    "\n",
    "state = 1\n",
    "accs = []\n",
    "\n",
    "while state < 100:\n",
    "    rf = RandomForestClassifier(max_depth = state)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    accs.append(accuracy_score(y_test, rf.predict(X_test)))\n",
    "    \n",
    "    state += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accs.index(max(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## This chunk of code builds the ensemble \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# This stores the weights that each model should have when voting\n",
    "weights = [ensemble.get_weights()[c] for c in ensemble.get_model_names()]\n",
    "\n",
    "# Here is where we might turn off different models due to lower accuracy score. \n",
    "# I don't know how the models will predict EBT\n",
    "# Might Turn off ANN\n",
    "weights[2] = 0\n",
    "\n",
    "#Also turn off Logistic Regression and Decision tree\n",
    "# weights[1] = 0\n",
    "# weights[-1] = 0\n",
    "\n",
    "vc_names = [('RF', rf), ('Logit', logit),\n",
    "('ANN', mlp), ('BRT', brt), ('DT', dt)]\n",
    "\n",
    "vc = VotingClassifier(estimators=vc_names, voting='soft', weights = weights)\n",
    "vc.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, vc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check out confusion matrix\n",
    "confusion_matrix(y_test, vc.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Model to Environmental Space\n",
    "#Here, we are interested in the \"pretty map\" habitat suitability indexes.\n",
    "first_decade = pd.read_csv(decade1, index_col = 'huc12')\n",
    "drops = [c for c in first_decade if c not in X_train.columns]\n",
    "bad_hucs = first_decade.index[first_decade.Max_LST_Annual.isna()]\n",
    "first_decade.drop(bad_hucs,inplace=True)\n",
    "first_decade.drop(columns = drops, inplace=True)\n",
    "\n",
    "\n",
    "#second_decade = pd.read_csv(decade2 ,index_col='huc12')\n",
    "#second_decade.drop(bad_hucs,inplace=True)\n",
    "#second_decade.drop(columns = drops, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MESS(train_df, pred_df):\n",
    "#     Let min_i be the minimum value of variable V_i\n",
    "#     over the reference point set, and similarly for max_i.\n",
    "    mins = dict(X_train.min())\n",
    "    maxs = dict(X_train.max())\n",
    "    \n",
    "    def calculate_s(column):\n",
    "        # Let f_i be the percent of reference points \n",
    "        # whose value of variable V_i is smaller than p_i.\n",
    "\n",
    "        # First store training values\n",
    "        values = train_df[column]\n",
    "\n",
    "        # Find f_i as above\n",
    "        sims = []\n",
    "        for element in np.array(pred_df[column]):\n",
    "            f = np.count_nonzero((values < element))/values.size\n",
    "\n",
    "            # Find Similarity:\n",
    "            if f == 0:\n",
    "                sim = ((element - mins[column]) / (maxs[column] - mins[column])) * 100\n",
    "\n",
    "            elif ((f > 0) & (f <= 50)):\n",
    "                sim = 2 * f\n",
    "\n",
    "            elif ((f > 50) & (f < 100)):\n",
    "                sim = 2 * (100-f)\n",
    "\n",
    "            elif f == 100:\n",
    "                sim = ((maxs[column] - element) / (maxs[column] - mins[column])) * 100\n",
    "\n",
    "            sims.append(sim)\n",
    "\n",
    "\n",
    "        return sims\n",
    "    \n",
    "    # Embedd Dataframe with sim values\n",
    "    sim_df = pd.DataFrame()\n",
    "    for c in pred_df.columns:\n",
    "        sim_df[c] = calculate_s(c)\n",
    "    \n",
    "    \n",
    "    minimum_similarity = sim_df.min(axis=1)\n",
    "    \n",
    "    return minimum_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET PREDICTION UNCERTAINTY\n",
    "predictions_first = []\n",
    "#predictions_second = []\n",
    "for ind,model in enumerate(vc.estimators_):\n",
    "    # Don't use Logistic Regression or MLP\n",
    "    if ind in [1,2]:\n",
    "        continue\n",
    "\n",
    "    predict_prob = [a[0] for a in model.predict_proba(first_decade)]\n",
    "    predictions_first.append(predict_prob)\n",
    "    \n",
    "    #predict_prob = [a[0] for a in model.predict_proba(second_decade)]\n",
    "    #predictions_second.append(predict_prob)\n",
    "\n",
    "## \\GET PREDICTION UNCERTAINTY\n",
    "    \n",
    "first_decade_pred = pd.DataFrame({'huc12':pd.Series(first_decade.index),\n",
    "                                  'prediction_ensemble':vc.predict(first_decade),\n",
    "                                  'prediction_proba':[a[0] for a in vc.predict_proba(first_decade)],\n",
    "                                  'prediction_uncertainty': np.std(np.array(predictions_first), axis=0),\n",
    "                                 'MESS': MESS(X_train,first_decade)})\n",
    "#second_decade_pred = pd.DataFrame({'huc12':pd.Series(second_decade.index),\n",
    "                                  #'prediction_ensemble':vc.predict(second_decade),\n",
    "                                  #'prediction_proba':[a[0] for a in vc.predict_proba(second_decade)],\n",
    "                                  #'prediction_uncertainty': np.std(np.array(predictions_second), axis=0),\n",
    "                                  #'MESS': MESS(X_train,second_decade)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge these predictions with the Shape File, which needs to be downloaded from the USGS website\n",
    "# This needs to be downloaded from the USGS website\n",
    "hucs = gpd.read_file('/Users/myles/Documents/flbs/MT_HUCS.geojson')\n",
    "hucs['huc12']=hucs['huc12'].astype('int64')\n",
    "\n",
    "#drops = ['MESS_mean', 'Most_Dissimilar_Variable_majority',\n",
    "       #'FIRST_Decade_QC_mean', 'Second_Decade_QC_mean', 'FIrst_Decade_QC_sum',\n",
    "       #'FIRST_DECADE_QC_sum', 'FIRST_DECADE_QC_max', 'SECOND_DECADE_QCmax']\n",
    "#hucs.drop(columns=drops,inplace=True)\n",
    "\n",
    "hucs_pred_first = hucs.merge(first_decade_pred,on='huc12')\n",
    "#hucs_pred_second = hucs.merge(second_decade_pred,on='huc12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_1 = pd.DataFrame(hucs_pred_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_1.to_csv(decade1_pred, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decade_2 = pd.DataFrame(hucs_pred_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decade_2.to_csv(decade2_pred, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This will be the backbone of the Community-level \"top predictors\" analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that runs the \"Drop Column\" Feature importance technique \n",
    "# I actually have these in a separate .py file which would be much cleaner. \n",
    "from sklearn.base import clone\n",
    "\n",
    "# Short function to create sorted data frame of feature importances\n",
    "import pandas as pd\n",
    "def make_imp_df(column_names, importances):\n",
    "    df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "def drop_col(model, X_train, y_train, random_state = 42):\n",
    "    #Clone the model\n",
    "    model_clone = clone(model)\n",
    "    #Reset random state\n",
    "    model_clone.random_state = random_state\n",
    "    #Train and score the benchmark model\n",
    "    model_clone.fit(X_train, y_train)\n",
    "    benchmark_score = model_clone.score(X_train, y_train)\n",
    "    #Store importances\n",
    "    importances = []\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        model_clone = clone(model)\n",
    "        model_clone.random_state = random_state\n",
    "        model_clone.fit(X_train.drop(col,axis=1),y_train)\n",
    "        drop_col_score = model_clone.score(X_train.drop(col, axis = 1), y_train)\n",
    "        importances.append(benchmark_score - drop_col_score)\n",
    "        \n",
    "    importances_df = make_imp_df(X_train.columns, importances)\n",
    "    return importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a bit of manipulation and run all these feature importance techniques\n",
    "vc_names = [('RF', rf), ('Logit', logit), ('ANN', mlp), ('BRT', brt), ('DT', dt)]\n",
    "def make_dict():\n",
    "    return dict(zip([tup[0] for tup in vc_names], [None]))\n",
    "\n",
    "\n",
    "rfe_dict = make_dict()\n",
    "perm_dict = make_dict()\n",
    "drop_dict = make_dict()\n",
    "\n",
    "for alg in vc.named_estimators:\n",
    "    dict_name = alg\n",
    "    clf = vc.named_estimators[dict_name]\n",
    "    \n",
    "    if dict_name == 'ANN':\n",
    "        continue\n",
    "    \n",
    "    print(\"Considering\", clf)\n",
    "    \n",
    "    # Find Recursive feature elimination for each classifier\n",
    "    rfe_selector = RFE(estimator=clf, n_features_to_select=3, step=1, verbose=5)\n",
    "    rfe_selector.fit(X_train,y_train)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_features = X_train.loc[:,rfe_support].columns.tolist()\n",
    "    \n",
    "    # Add to rfe_dict\n",
    "    rfe_dict[dict_name] = rfe_features\n",
    "    \n",
    "    \n",
    "    # //========================================================\n",
    "    # Find Permuation importance for each classifier\n",
    "    perm_imp = permutation_importance(clf, X_train, y_train,\n",
    "                          n_repeats=30,\n",
    "                          random_state = 0)\n",
    "    \n",
    "    perm_imp['feature'] = X_train.columns\n",
    "    perm_features = pd.DataFrame(perm_imp['feature'],perm_imp['importances_mean'],columns = ['Feature']) \\\n",
    "                            .sort_index(ascending=False)['Feature'].values[:3]\n",
    "    \n",
    "    # Add permutation features to dict\n",
    "    perm_dict[dict_name] = perm_features\n",
    "    \n",
    "    \n",
    "    \n",
    "    #//========================================================\n",
    "    # Find Drop Columns importance for each classifier\n",
    "    drop_col_feats = drop_col(clf, X_train, y_train, random_state = 10)\n",
    "    drop_col_three = drop_col_feats.sort_values('feature_importance',ascending = False)['feature'][:3]\n",
    "    \n",
    "    drop_dict[dict_name] = drop_col_three\n",
    "    \n",
    "    \n",
    "print(\"Done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_true(series):\n",
    "    return [True if feature in series else False for feature in X_train.columns]\n",
    "\n",
    "def rename_dict(dictionary, tek_name):\n",
    "    return_names = []\n",
    "    return_lists = []\n",
    "    \n",
    "    for item in dictionary.items():\n",
    "        return_names.append(tek_name + str(item[0]))\n",
    "        return_lists.append(mark_true(list(item[1])))\n",
    "        \n",
    "    return dict(zip(return_names, return_lists))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We end up with a dataframe that says, for any model / feature importance technique, whether or not a feature ended up in the top N (i.e. whether or not that feature is important).\n",
    "features_df = pd.concat([pd.DataFrame(rename_dict(perm_dict,'PERM_')).reset_index(drop=True),\n",
    "                        pd.DataFrame(rename_dict(rfe_dict,'RFE_')),\n",
    "                        pd.DataFrame(rename_dict(drop_dict, 'DROP_'))],axis=1)\n",
    "\n",
    "\n",
    "features_df['Feature'] = X_train.columns\n",
    "features_df['Total'] = np.sum(features_df, axis=1)\n",
    "features_df.sort_values(['Total','Feature'] , ascending=False,inplace=True)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plt.setp( ax.xaxis.get_majorticklabels(), rotation=-45, ha=\"left\" )\n",
    "plt.setp( ax.xaxis.get_majorticklabels(), rotation=-45, ha=\"left\" )\n",
    "\n",
    "# plt.xticks(rotation=75)\n",
    "plt.bar(features_df['Feature'], features_df['Total'])\n",
    "plt.title(\"Variable importances\")\n",
    "plt.ylabel(\"Number of times a variable appeared in top 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "That's it. These are the results we are looking for regarding the NAS community-level study."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.716px",
    "left": "1374.44px",
    "right": "20px",
    "top": "59.9943px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
