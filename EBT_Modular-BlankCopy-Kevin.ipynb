{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Meeting Notes:\n",
    "    Modularity \n",
    "    Data issues: \n",
    "        Availability of absence data\n",
    "        Gaps between year of available data\n",
    "    Best platform for end-user tool\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Environment Set up\n",
    "\n",
    "conda create -n gee python=3\n",
    "source activate gee\n",
    "conda install -c conda-forge earthengine-api\n",
    "conda install -c anaconda pandas\n",
    "#After installing the Python GEE API, you have to run earthengine authenticate in the terminal and follow the directions. This will connect the API to your google account.\n",
    "\n",
    "#After running earthengine authenticate, you should have an environment variable set up with an authentication key, which allows you to directly initialize EE without authenticating each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import ee and required packages\n",
    "import ee\n",
    "ee.Initialize()\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "\n",
    "from src.gee_funs import *\n",
    "import src.build_annual_cube as bac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLE DECLARATIONS\n",
    "\n",
    "STATE=\"Montana\"\n",
    "state_abbrevs = {\n",
    "    'Montana' : 'MT'\n",
    "}\n",
    "\n",
    "start_year = 2002\n",
    "end_year = 2018\n",
    "\n",
    "gee_path='users/kjchristensen93/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Modular Variables:\n",
    "\n",
    "#If you have a spatially thinned data set, start here after initializing ee\n",
    "\n",
    "#Taxa thinned dataset\n",
    "SThin = ee.FeatureCollection('users/kjchristensen93/EBT_data/EBT_SThin')\n",
    "#Study dates\n",
    "#Note we are limited to 2002 - 2018 due to the water year covariate \n",
    "\n",
    "### Returns a list of dates from 2002 - 2018 ###\n",
    "years = range(start_year,end_year) \n",
    "s_dates = ee.List(list(map(lambda x: ee.Date(str(x) + '-01-01'), years)))\n",
    "\n",
    "#HUC state geojson file \n",
    "HUC_state = ('./datasets/hucs/MT_HUCS.geojson')\n",
    "#Define export locations:\n",
    "#GEE yearly covariate folder\n",
    "assetId = (gee_path+'covariates/covariates_test') \n",
    "#User training csv local directory folder\n",
    "trainingdata = ('./datasets/training/')\n",
    "#User decadal image local directory folder\n",
    "decadalfolder = ('./datasets/decade/')\n",
    "#Define export naming convention? Maybe we define a function within code above for naming conventions\n",
    "\n",
    "\n",
    "#### ML Variables ####\n",
    "\n",
    "#Training Glob\n",
    "trainingglob = ('./datasets/training/*.csv')\n",
    "# trainingglob = ((trainingdata)/*.csv) will this work?\n",
    "#decadal CSV directory and naming conventions\n",
    "decade1 = ('./datasets/decade/decade1_filename.csv')\n",
    "decade2 =('./datasets/decade/decade2_filename.csv')\n",
    "#decadal predictions\n",
    "decade1_pred = ('./datasets/decade/decade1_pred_filename.csv')\n",
    "decade2_pred = ('./datasets/decade/decade2_pred_filename.csv')\n",
    "\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#If you need to create the spatially thinned asset...Otherwise skip to Define Modular Variables below\n",
    "#Define GEE asset/location of desired dataset (Formatted CSV must be uploaded to your GEE assets with Lat/Long columns defined \n",
    "#before starting)\n",
    "Taxa_og = ee.FeatureCollection(gee_path+'EBT_data/EBT_mfish_data_presence_heuristic')\n",
    "coll = ee.FeatureCollection(Taxa_og) \n",
    "distance = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Spatially thin locations and export to asset\n",
    "# Performs the spatial thinning algorithm on each year separately\n",
    "feats = s_dates.map(lambda x: filter_date_space(x,coll,distance))\n",
    "\n",
    "# Combine each of the resultant filtered collections\n",
    "first = ee.FeatureCollection(Taxa_og)\n",
    "spatially_thin = ee.FeatureCollection(feats.iterate(merge_coll, first))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "export3 = ee.batch.Export.table.toAsset(collection = spatially_thin,\n",
    "                    description = 'EBT_SThin', # n<-------- CHANGE NAME FOR DIFFERENT DATA\n",
    "                    assetId = gee_path+'EBT_data/EBT_SThin') # <----- CHANGE Export location FOR DIFFERENT USER\n",
    "\n",
    "export3.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This list dictates what years will be exported for both the Yearly Covariate Images and the Yearly Training CSVS\n",
    "# can this be changed to a list for intermitent datasets missing years? Empty outputs causes issues later on....\n",
    "import time\n",
    "# Enter start year for Y and end year for Y\n",
    "years = [str(y) for y in list(range(2002, 2005))]  ##FIXME: hardcoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Export data using python API magic\n",
    "# Define geometry by changing state name so we can export the whole state at once\n",
    "states = ee.FeatureCollection(\"TIGER/2016/States\")\n",
    "#Enter state 2-digit abbreviation for study area\n",
    "geometry = states.filter(ee.Filter.eq('NAME',STATE)).geometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape file containing HUC polygons\n",
    "HUC = ee.FeatureCollection(\"USGS/WBD/2017/HUC12\")\n",
    "# Choose state to clip HUC by. Change Abbreviation to match dataset \n",
    "#Enter state full name for X (i.e., Illinois/ look at dataset for formats for this stuff)\n",
    "HUC_clip = HUC.filter(ee.Filter.eq('states',state_abbrevs[STATE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed observation Year as system:start_time for thinned dataset \n",
    "# We have had to add this \"Year Column\" manually to the datasets.  Make sure your dataset has correct column headings\n",
    "SThin_map = SThin.map(embedd_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Define helper filters and lists to iterate over\n",
    "#========================================================\n",
    "# Build Lists from which to map over\n",
    "#========================================================\n",
    "# List from which absences will be built\n",
    "ee_dates = ee.List(s_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#========================================================\n",
    "# Run covariate algorithm and build a list of images\n",
    "# with each image corresponding to each year and each band corresponding to each covariate\n",
    "#========================================================\n",
    "\n",
    "banded_images_list = bac.build_all_cubes(start_year, end_year)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Skip this step if you already have them stored in GEE\n",
    "#Export Yearly Covariate Images\n",
    "\n",
    "# Export each image within the for loop\n",
    "for i,y in zip(range(len(years)), years):\n",
    "    print(\"Starting\", y)\n",
    "    img = ee.Image(ee.List(banded_images_list).get(ee.Number(i)))\n",
    "    export = ee.batch.Export.image.toAsset(image = img,\n",
    "                    description = 'covariate_'+y,\n",
    "                    assetId = (assetId) +y, \n",
    "                    region = ee.Geometry(geometry),\n",
    "                    scale =  100,\n",
    "                    maxPixels = 1e13)\n",
    "    export.start()\n",
    "    \n",
    "    print(y,\"status:    \", export.status()['state'])\n",
    "\n",
    "    # Wait for 30 seconds so that the export['state'] gives insightful information\n",
    "    time.sleep(15)\n",
    "    print(y,\"status:    \", export.status()['state'])\n",
    "    \n",
    "    \n",
    "    # If this status is \"RUNNING\", then there are no egretious syntax errors. \n",
    "    # However, it is still possible that these export commands fail after more than 30 seconds.\n",
    "    # In that case, it is likely that there is a Computation Time Out Error (remember exporting the annual stacks)\n",
    "    time.sleep(15)\n",
    "    print(y,\"status:    \", export.status()['state'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Start Here if you have yearly covariates created\n",
    "\n",
    "#Export training CSVs\n",
    "## Reduce Regions from existing images\n",
    "\n",
    "# COVARIATE IMAGES  \n",
    "\n",
    "path = assetId\n",
    "years = range(start_year, 2005)\n",
    "images = list(map(lambda x: ee.Image(path + str(x)), years))\n",
    "banded_images_asset_list = ee.List(images)\n",
    "\n",
    "for i in range(len(years)):\n",
    "    print(\"Starting\", i)\n",
    "    \n",
    "    img = ee.Image(banded_images_asset_list.get(i))\n",
    "    data = reduce_HUCS(img,SThin_map,HUC_clip) \n",
    "    \n",
    "    ## PYTHON API MAGIC!! LOOK HERE\n",
    "    my_csv = pd.DataFrame([x['properties'] for x in data.getInfo()['features']])\n",
    "    \n",
    "    # From there, we can write it directly to our directory and stitch it together afterwards\n",
    "    my_csv.to_csv((trainingdata) + str(2002+i) + '.csv', index=False) \n",
    "    print(\"Finished\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export the information that we will use to project habitat suitability. \n",
    "#Decades were convenient for RBT, but not other taxa with less data/ we can change.\n",
    "# Change to match dataset\n",
    "\n",
    "#Can we set this up such that this is automatically defined when we define the year range above?\n",
    "first_decade = ee.ImageCollection.fromImages(images[0:7]).mean()\n",
    "\n",
    "#second_decade = ee.ImageCollection.fromImages(images[7:]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Export these data as csvs\n",
    "first_decade_img = ee.Image(first_decade)\n",
    "\n",
    "first_csv = first_decade_img.reduceRegions(**{\n",
    "                              'collection': HUC_clip,\n",
    "                              'reducer': ee.Reducer.mean(),\n",
    "                              'crs': 'EPSG:4326',\n",
    "                              'scale': 100,\n",
    "                              'tileScale': 16})\n",
    "\n",
    "#PYTHON API MAGIC!! LOOK HERE\n",
    "first_decade_data = pd.DataFrame([x['properties'] for x in first_csv.getInfo()['features']])\n",
    "\n",
    "# From there, we can write it directly to our directory and stitch it together afterwards\n",
    "#maybe we should think about 2 and 5 year bins due to limitations of datasets for some taxa/ to make more useful for managers\n",
    "first_decade_data.to_csv(decade1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Export these data as csvs for second decade if dataset contains that many sampling years\n",
    "second_decade_img = ee.Image(second_decade)\n",
    "\n",
    "second_csv = second_decade_img.reduceRegions(**{\n",
    "                              'collection': HUC_clip,\n",
    "                              'reducer': ee.Reducer.mean(),\n",
    "                              'crs': 'EPSG:4326',\n",
    "                              'scale': 100,\n",
    "                              'tileScale': 16})\n",
    "\n",
    "## PYTHON API MAGIC!! LOOK HERE\n",
    "second_decade_data = pd.DataFrame([x['properties'] for x in second_csv.getInfo()['features']])\n",
    "\n",
    "# From there, we can write it directly to our directory and stitch it together afterwards\n",
    "second_decade_data.to_csv(decade2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined data\n",
    "\n",
    "dfs = []\n",
    "for file in glob.glob(trainingglob):\n",
    "    dfs.append(pd.read_csv(file))\n",
    "\n",
    "df = pd.DataFrame(pd.concat(dfs, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Points', 'areaacres', 'areasqkm', \n",
    "           'gnis_id','huc12', 'humod','hutype',\n",
    "           'loaddate', 'metasource', 'name', \n",
    "           'noncontr00','noncontrib','shape_area',\n",
    "           'shape_leng', 'sourcedata','sourcefeat',\n",
    "           'sourceorig','states','tnmid', 'tohuc', 'Time']\n",
    "\n",
    "df['Avg_Presence'] = [1 if x > 0 else 0 for x in df['Avg_Presence']]\n",
    "df.drop(columns = to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ### ML STARTS HERE!!!!! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Train Models\n",
    "\n",
    "#install skLearn if you don't already have it\n",
    "\n",
    "# Ensemble modeling class\n",
    "from sklearn.metrics import *\n",
    "class Ensemble():\n",
    "    def __init__(self, models = []):\n",
    "        self.models = models\n",
    "        self.accs_dict = None\n",
    "        self.rocs_dict = None\n",
    "        self.model_names = [m.__class__.__name__ for m in models]\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit_all(self, X_train, y_train):\n",
    "        for m in self.models:\n",
    "            print(\"Fitting\", m.__class__.__name__)\n",
    "            m.fit(X_train, y_train)\n",
    "            print(m.__class__.__name__, 'fit.')\n",
    "        \n",
    "    \n",
    "    def evaluate_all(self, X_test, y_true, metric = 'acc'):\n",
    "        accs = [accuracy_score(y_true, m.predict(X_test)) for m in self.models]\n",
    "        accs_dict = dict(zip(self.model_names, accs))\n",
    "        \n",
    "        \n",
    "        rocs = [roc_auc_score(y_true, m.predict(X_test)) for m in self.models]\n",
    "        rocs_dict = dict(zip(self.model_names, rocs))\n",
    "        \n",
    "        self.rocs_dict = rocs_dict\n",
    "        self.accs_dict = accs_dict\n",
    "        \n",
    "        if metric == 'acc':\n",
    "            return accs_dict\n",
    "        \n",
    "        return rocs_dict\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.rocs_dict\n",
    "    \n",
    "    def get_model_names(self):\n",
    "        return self.model_names\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = ['Avg_Presence']),\n",
    "                                                   df['Avg_Presence'], random_state = 73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "\n",
    "# Here is the ensemble of models - again, I think the tree methods are far stronger\n",
    "mlp = MLPClassifier(max_iter = 1000, random_state = 73)\n",
    "logit = LogisticRegression(max_iter = 10000)\n",
    "rf = RandomForestClassifier()\n",
    "brt = GradientBoostingClassifier()\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Construct ensemble object\n",
    "ensemble = Ensemble([mlp, brt, dt, rf, logit]) \n",
    "ensemble.fit_all(X_train,y_train)\n",
    "\n",
    "print(ensemble.evaluate_all(X_test, y_test, metric = 'roc'))\n",
    "#y_pred = ensemble.evaluate_ensemble(X_test, y_test)\n",
    "\n",
    "#accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Test classifyers for ideal settings with this function: Change model names and parameters\n",
    "#n_estimators doesn't affect score\n",
    "\n",
    "state = 1\n",
    "accs = []\n",
    "\n",
    "while state < 100:\n",
    "    rf = RandomForestClassifier(max_depth = state)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    accs.append(accuracy_score(y_test, rf.predict(X_test)))\n",
    "    \n",
    "    state += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accs.index(max(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## This chunk of code builds the ensemble \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# This stores the weights that each model should have when voting\n",
    "weights = [ensemble.get_weights()[c] for c in ensemble.get_model_names()]\n",
    "\n",
    "# Here is where we might turn off different models due to lower accuracy score. \n",
    "# I don't know how the models will predict EBT\n",
    "# Might Turn off ANN\n",
    "weights[2] = 0\n",
    "\n",
    "#Also turn off Logistic Regression and Decision tree\n",
    "# weights[1] = 0\n",
    "# weights[-1] = 0\n",
    "\n",
    "vc_names = [('RF', rf), ('Logit', logit),\n",
    "('ANN', mlp), ('BRT', brt), ('DT', dt)]\n",
    "\n",
    "vc = VotingClassifier(estimators=vc_names, voting='soft', weights = weights)\n",
    "vc.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, vc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check out confusion matrix\n",
    "confusion_matrix(y_test, vc.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Model to Environmental Space\n",
    "#Here, we are interested in the \"pretty map\" habitat suitability indexes.\n",
    "first_decade = pd.read_csv(decade1, index_col = 'huc12')\n",
    "drops = [c for c in first_decade if c not in X_train.columns]\n",
    "bad_hucs = first_decade.index[first_decade.Max_LST_Annual.isna()]\n",
    "first_decade.drop(bad_hucs,inplace=True)\n",
    "first_decade.drop(columns = drops, inplace=True)\n",
    "\n",
    "\n",
    "#second_decade = pd.read_csv(decade2 ,index_col='huc12')\n",
    "#second_decade.drop(bad_hucs,inplace=True)\n",
    "#second_decade.drop(columns = drops, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MESS(train_df, pred_df):\n",
    "#     Let min_i be the minimum value of variable V_i\n",
    "#     over the reference point set, and similarly for max_i.\n",
    "    mins = dict(X_train.min())\n",
    "    maxs = dict(X_train.max())\n",
    "    \n",
    "    def calculate_s(column):\n",
    "        # Let f_i be the percent of reference points \n",
    "        # whose value of variable V_i is smaller than p_i.\n",
    "\n",
    "        # First store training values\n",
    "        values = train_df[column]\n",
    "\n",
    "        # Find f_i as above\n",
    "        sims = []\n",
    "        for element in np.array(pred_df[column]):\n",
    "            f = np.count_nonzero((values < element))/values.size\n",
    "\n",
    "            # Find Similarity:\n",
    "            if f == 0:\n",
    "                sim = ((element - mins[column]) / (maxs[column] - mins[column])) * 100\n",
    "\n",
    "            elif ((f > 0) & (f <= 50)):\n",
    "                sim = 2 * f\n",
    "\n",
    "            elif ((f > 50) & (f < 100)):\n",
    "                sim = 2 * (100-f)\n",
    "\n",
    "            elif f == 100:\n",
    "                sim = ((maxs[column] - element) / (maxs[column] - mins[column])) * 100\n",
    "\n",
    "            sims.append(sim)\n",
    "\n",
    "\n",
    "        return sims\n",
    "    \n",
    "    # Embedd Dataframe with sim values\n",
    "    sim_df = pd.DataFrame()\n",
    "    for c in pred_df.columns:\n",
    "        sim_df[c] = calculate_s(c)\n",
    "    \n",
    "    \n",
    "    minimum_similarity = sim_df.min(axis=1)\n",
    "    \n",
    "    return minimum_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET PREDICTION UNCERTAINTY\n",
    "predictions_first = []\n",
    "#predictions_second = []\n",
    "for ind,model in enumerate(vc.estimators_):\n",
    "    # Don't use Logistic Regression or MLP\n",
    "    if ind in [1,2]:\n",
    "        continue\n",
    "\n",
    "    predict_prob = [a[0] for a in model.predict_proba(first_decade)]\n",
    "    predictions_first.append(predict_prob)\n",
    "    \n",
    "    #predict_prob = [a[0] for a in model.predict_proba(second_decade)]\n",
    "    #predictions_second.append(predict_prob)\n",
    "\n",
    "## \\GET PREDICTION UNCERTAINTY\n",
    "    \n",
    "first_decade_pred = pd.DataFrame({'huc12':pd.Series(first_decade.index),\n",
    "                                  'prediction_ensemble':vc.predict(first_decade),\n",
    "                                  'prediction_proba':[a[0] for a in vc.predict_proba(first_decade)],\n",
    "                                  'prediction_uncertainty': np.std(np.array(predictions_first), axis=0),\n",
    "                                 'MESS': MESS(X_train,first_decade)})\n",
    "#second_decade_pred = pd.DataFrame({'huc12':pd.Series(second_decade.index),\n",
    "                                  #'prediction_ensemble':vc.predict(second_decade),\n",
    "                                  #'prediction_proba':[a[0] for a in vc.predict_proba(second_decade)],\n",
    "                                  #'prediction_uncertainty': np.std(np.array(predictions_second), axis=0),\n",
    "                                  #'MESS': MESS(X_train,second_decade)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge these predictions with the Shape File, which needs to be downloaded from the USGS website\n",
    "# This needs to be downloaded from the USGS website\n",
    "hucs = gpd.read_file(HUC_state)\n",
    "hucs['huc12']=hucs['huc12'].astype('int64')\n",
    "\n",
    "#drops = ['MESS_mean', 'Most_Dissimilar_Variable_majority',\n",
    "       #'FIRST_Decade_QC_mean', 'Second_Decade_QC_mean', 'FIrst_Decade_QC_sum',\n",
    "       #'FIRST_DECADE_QC_sum', 'FIRST_DECADE_QC_max', 'SECOND_DECADE_QCmax']\n",
    "#hucs.drop(columns=drops,inplace=True)\n",
    "\n",
    "hucs_pred_first = hucs.merge(first_decade_pred,on='huc12')\n",
    "#hucs_pred_second = hucs.merge(second_decade_pred,on='huc12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_1 = pd.DataFrame(hucs_pred_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_1.to_csv(decade1_pred, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decade_2 = pd.DataFrame(hucs_pred_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decade_2.to_csv(decade2_pred, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be the backbone of the Community-level \"top predictors\" analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that runs the \"Drop Column\" Feature importance technique \n",
    "# I actually have these in a separate .py file which would be much cleaner. \n",
    "from sklearn.base import clone\n",
    "\n",
    "# Short function to create sorted data frame of feature importances\n",
    "import pandas as pd\n",
    "def make_imp_df(column_names, importances):\n",
    "    df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "def drop_col(model, X_train, y_train, random_state = 42):\n",
    "    #Clone the model\n",
    "    model_clone = clone(model)\n",
    "    #Reset random state\n",
    "    model_clone.random_state = random_state\n",
    "    #Train and score the benchmark model\n",
    "    model_clone.fit(X_train, y_train)\n",
    "    benchmark_score = model_clone.score(X_train, y_train)\n",
    "    #Store importances\n",
    "    importances = []\n",
    "    \n",
    "    for col in X_train.columns:\n",
    "        model_clone = clone(model)\n",
    "        model_clone.random_state = random_state\n",
    "        model_clone.fit(X_train.drop(col,axis=1),y_train)\n",
    "        drop_col_score = model_clone.score(X_train.drop(col, axis = 1), y_train)\n",
    "        importances.append(benchmark_score - drop_col_score)\n",
    "        \n",
    "    importances_df = make_imp_df(X_train.columns, importances)\n",
    "    return importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import RFE, SelectFromModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a bit of manipulation and run all these feature importance techniques\n",
    "vc_names = [('RF', rf), ('Logit', logit), ('ANN', mlp), ('BRT', brt), ('DT', dt)]\n",
    "def make_dict():\n",
    "    return dict(zip([tup[0] for tup in vc_names], [None]))\n",
    "\n",
    "\n",
    "rfe_dict = make_dict()\n",
    "perm_dict = make_dict()\n",
    "drop_dict = make_dict()\n",
    "\n",
    "for alg in vc.named_estimators:\n",
    "    dict_name = alg\n",
    "    clf = vc.named_estimators[dict_name]\n",
    "    \n",
    "    if dict_name == 'ANN':\n",
    "        continue\n",
    "    \n",
    "    print(\"Considering\", clf)\n",
    "    \n",
    "    # Find Recursive feature elimination for each classifier\n",
    "    rfe_selector = RFE(estimator=clf, n_features_to_select=3, step=1, verbose=5)\n",
    "    rfe_selector.fit(X_train,y_train)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_features = X_train.loc[:,rfe_support].columns.tolist()\n",
    "    \n",
    "    # Add to rfe_dict\n",
    "    rfe_dict[dict_name] = rfe_features\n",
    "    \n",
    "    \n",
    "    # //========================================================\n",
    "    # Find Permuation importance for each classifier\n",
    "    perm_imp = permutation_importance(clf, X_train, y_train,\n",
    "                          n_repeats=30,\n",
    "                          random_state = 0)\n",
    "    \n",
    "    perm_imp['feature'] = X_train.columns\n",
    "    perm_features = pd.DataFrame(perm_imp['feature'],perm_imp['importances_mean'],columns = ['Feature']) \\\n",
    "                            .sort_index(ascending=False)['Feature'].values[:3]\n",
    "    \n",
    "    # Add permutation features to dict\n",
    "    perm_dict[dict_name] = perm_features\n",
    "    \n",
    "    \n",
    "    \n",
    "    #//========================================================\n",
    "    # Find Drop Columns importance for each classifier\n",
    "    drop_col_feats = drop_col(clf, X_train, y_train, random_state = 10)\n",
    "    drop_col_three = drop_col_feats.sort_values('feature_importance',ascending = False)['feature'][:3]\n",
    "    \n",
    "    drop_dict[dict_name] = drop_col_three\n",
    "    \n",
    "    \n",
    "print(\"Done\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_true(series):\n",
    "    return [True if feature in series else False for feature in X_train.columns]\n",
    "\n",
    "def rename_dict(dictionary, tek_name):\n",
    "    return_names = []\n",
    "    return_lists = []\n",
    "    \n",
    "    for item in dictionary.items():\n",
    "        return_names.append(tek_name + str(item[0]))\n",
    "        return_lists.append(mark_true(list(item[1])))\n",
    "        \n",
    "    return dict(zip(return_names, return_lists))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We end up with a dataframe that says, for any model / feature importance technique, whether or not a feature ended up in the top N (i.e. whether or not that feature is important).\n",
    "features_df = pd.concat([pd.DataFrame(rename_dict(perm_dict,'PERM_')).reset_index(drop=True),\n",
    "                        pd.DataFrame(rename_dict(rfe_dict,'RFE_')),\n",
    "                        pd.DataFrame(rename_dict(drop_dict, 'DROP_'))],axis=1)\n",
    "\n",
    "\n",
    "features_df['Feature'] = X_train.columns\n",
    "features_df['Total'] = np.sum(features_df, axis=1)\n",
    "features_df.sort_values(['Total','Feature'] , ascending=False,inplace=True)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plt.setp( ax.xaxis.get_majorticklabels(), rotation=-45, ha=\"left\" )\n",
    "plt.setp( ax.xaxis.get_majorticklabels(), rotation=-45, ha=\"left\" )\n",
    "\n",
    "# plt.xticks(rotation=75)\n",
    "plt.bar(features_df['Feature'], features_df['Total'])\n",
    "plt.title(\"Variable importances\")\n",
    "plt.ylabel(\"Number of times a variable appeared in top 3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. These are the results we are looking for regarding the NAS community-level study."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.716px",
    "left": "1374.44px",
    "right": "20px",
    "top": "59.9943px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
